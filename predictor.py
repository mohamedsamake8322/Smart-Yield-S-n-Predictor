import torch
import torch.nn as nn
import pandas as pd
import os
import logging
from sklearn.preprocessing import StandardScaler

# âœ… DÃ©finition du pÃ©riphÃ©rique (CPU uniquement)
device = torch.device("cpu")

# âœ… Configuration du logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ---------- DÃ©tection automatique des colonnes ----------
def detect_input_size(csv_path="data.csv"):
    df = pd.read_csv(csv_path)
    logging.info(f"ðŸ”Ž Colonnes disponibles dans le dataset : {df.columns.tolist()}")

    if "yield" not in df.columns:
        raise KeyError("ðŸ›‘ Erreur : La colonne 'yield' n'existe pas dans le dataset. VÃ©rifie ton fichier CSV.")

    input_size = len(df.columns) - 1

    try:
        input_size = int(input_size)  # âœ… Convertir en entier pour Ã©viter lâ€™erreur
    except ValueError:
        logging.error(f"ðŸ›‘ Erreur : `input_size` doit Ãªtre un entier, mais reÃ§u {type(input_size)}")
        raise TypeError(f"input_size must be an integer, but got {type(input_size)}")

    logging.info(f"âœ… DÃ©tection des features : {input_size} colonnes utilisÃ©es pour la prÃ©diction.")
    return input_size, df


# ---------- DÃ©finition du modÃ¨le PyTorch ----------
class PyTorchModel(nn.Module):
    def __init__(self, input_size):
        super(PyTorchModel, self).__init__()

        self.fc1 = nn.Linear(input_size, 64).to(device)
        self.batch_norm1 = nn.BatchNorm1d(64)  # âœ… Ajout de BatchNorm
        self.fc2 = nn.Linear(64, 32).to(device)
        self.batch_norm2 = nn.BatchNorm1d(32)  # âœ… Ajout de BatchNorm
        self.fc3 = nn.Linear(32, 1).to(device)

        self.activation = nn.LeakyReLU(negative_slope=0.01)  # âœ… Activation amÃ©liorÃ©e
        self.dropout = nn.Dropout(0.3)  # âœ… RÃ©gularisation pour Ã©viter l'overfitting

    def forward(self, x):
        x = x.to(device)
        x = self.activation(self.batch_norm1(self.fc1(x)))
        x = self.dropout(x)
        x = self.activation(self.batch_norm2(self.fc2(x)))
        x = self.dropout(x)
        x = self.fc3(x)
        return x
# ---------- Model Persistence ----------
MODEL_PATH = "model/disease_model.pth"

def load_model(input_size, path=MODEL_PATH):
    """Charge le modÃ¨le PyTorch et vÃ©rifie la compatibilitÃ© avec `input_size`."""
    model = PyTorchModel(input_size)
    
    if not os.path.exists(path):
        logging.error(f"ðŸš« ModÃ¨le non trouvÃ© Ã  {path}. VÃ©rifie que l'entraÃ®nement a bien eu lieu.")
        raise FileNotFoundError(f"ModÃ¨le non trouvÃ© : {path}")
    
    try:
        model.load_state_dict(torch.load(path, map_location=device))
        model.eval()
        logging.info(f"âœ… ModÃ¨le PyTorch chargÃ© avec succÃ¨s depuis {path} !")
    except RuntimeError as e:
        logging.error(f"ðŸ›‘ Erreur de chargement du modÃ¨le : {e}")
        raise RuntimeError("Le fichier du modÃ¨le n'est pas compatible avec l'architecture actuelle.")
    
    return model

def save_model(model, path=MODEL_PATH):
    """Sauvegarde le modÃ¨le PyTorch."""
    try:
        torch.save(model.state_dict(), path)
        logging.info(f"âœ… ModÃ¨le PyTorch sauvegardÃ© sous {path}.")
    except Exception as e:
        logging.error(f"ðŸ›‘ Erreur lors de la sauvegarde du modÃ¨le : {e}")
        raise RuntimeError("Impossible de sauvegarder le modÃ¨le.")

# ---------- Nettoyage et Normalisation du CSV ----------
def clean_and_normalize_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """Nettoie et normalise les donnÃ©es pour Ã©viter les erreurs avec PyTorch."""
    logging.info("ðŸ”„ VÃ©rification et normalisation du dataset...")

    df = df.apply(pd.to_numeric, errors="coerce")
    df.fillna(0, inplace=True)

    scaler = StandardScaler()
    df[df.columns] = scaler.fit_transform(df[df.columns])  # âœ… Normalisation
    logging.info("âœ… Normalisation terminÃ©e.")

    return df

# ---------- Single Prediction ----------
def predict_single(model, features: dict):
    """Effectue une prÃ©diction unique."""
    input_df = pd.DataFrame([features])
    input_df = clean_and_normalize_dataframe(input_df)

    input_tensor = torch.tensor(input_df.values, dtype=torch.float32).to(device)
    prediction = model(input_tensor).item()
    return prediction

# ---------- Batch Prediction ----------
def predict_batch(model, df: pd.DataFrame):
    """Effectue des prÃ©dictions sur plusieurs donnÃ©es."""
    df = clean_and_normalize_dataframe(df)

    required_features = list(df.columns)
    input_tensor = torch.tensor(df[required_features].values, dtype=torch.float32).to(device)
    
    predictions = model(input_tensor).detach().numpy()
    return predictions

# ---------- ExÃ©cution autonome du script ----------
if __name__ == "__main__":
    logging.info("ðŸ”„ DÃ©tection automatique des features...")
    try:
        input_size, df = detect_input_size()
    except KeyError as e:
        logging.error(str(e))
        exit(1)

    # ðŸš¨ VÃ©rification finale du type de `input_size`
    if not isinstance(input_size, int):
        logging.error(f"ðŸ›‘ `input_size` doit Ãªtre un entier, mais reÃ§u {type(input_size)} avec valeur `{input_size}`")
        raise TypeError(f"`input_size` must be an integer, but got {type(input_size)}")

    logging.info("ðŸ”„ Chargement du modÃ¨le...")
    model = load_model(input_size=input_size)

    # ðŸ”¥ Test rapide de prÃ©diction avec des valeurs fictives
    example_features = {
        "Temperature": 25,
        "Humidity": 60,
        "Precipitation": 12,
        "pH": 6.5,
        "Fertilizer": 80,
        "NDVI": 0.5
    }
    prediction = predict_single(model, example_features)
    logging.info(f"ðŸŒ¾ PrÃ©diction du rendement: {prediction:.2f} tonnes/hectare")
